{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import make_epsilon_greedy_policy\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from integrated_agent.dqn_agent import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your environment\n",
    "env = gym.make(\"MinAtar/Breakout-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Print observation and action space infos\n",
    "print(f\"Training on {env.spec.id}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters, Hint: Change as you see fit\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_BUFFER_SIZE = 100_000\n",
    "UPDATE_FREQ = 100\n",
    "EPS_START = 1\n",
    "EPS_END = 0.01\n",
    "SCHEDULE_DURATION = 15_000\n",
    "NUM_EPISODES = 1_000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_STEPS = 3\n",
    "\n",
    "ALPHA = 0.5\n",
    "BETA = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DQN\n",
    "agent = DQNAgent(\n",
    "    env,\n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    eps_start=EPS_START,\n",
    "    eps_end=EPS_END,\n",
    "    schedule_duration=SCHEDULE_DURATION,\n",
    "    update_freq=UPDATE_FREQ,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    "    alpha=ALPHA,\n",
    "    beta=BETA,\n",
    ")\n",
    "\n",
    "results = agent.train(NUM_EPISODES, N_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = results[0]\n",
    "\n",
    "smoothing_window = 20\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "# Plot the episode length over time\n",
    "ax = axes[0]\n",
    "ax.plot(stats.episode_lengths)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Length\")\n",
    "ax.set_title(\"Episode Length over Time\")\n",
    "\n",
    "# Plot the episode reward over time\n",
    "ax = axes[1]\n",
    "rewards_smoothed = (\n",
    "    pd.Series(stats.episode_rewards)\n",
    "    .rolling(smoothing_window, min_periods=smoothing_window)\n",
    "    .mean()\n",
    ")\n",
    "ax.plot(rewards_smoothed)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
    "ax.set_title(\n",
    "    f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "\n",
    "\n",
    "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
    "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
    "    # Create a list to hold each frame\n",
    "    frames = []\n",
    "\n",
    "    # Convert RGB arrays to PIL Image objects\n",
    "    for rgb_array in rgb_arrays:\n",
    "        rgb_array = (rgb_array * 255).astype(np.uint8)\n",
    "        rgb_array = rgb_array.repeat(48, axis=0).repeat(48, axis=1)\n",
    "        img = Image.fromarray(rgb_array)\n",
    "        frames.append(img)\n",
    "\n",
    "    # Save the frames as an animated GIF\n",
    "    frames[0].save(\n",
    "        filename, save_all=True, append_images=frames[1:], duration=duration, loop=0\n",
    "    )\n",
    "\n",
    "\n",
    "def rendered_rollout(policy, env, max_steps=1_000):\n",
    "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = [env.render()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = policy(torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0))\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        imgs.append(env.render())\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "policy = make_epsilon_greedy_policy(agent.q, num_actions=env.action_space.n)\n",
    "imgs = rendered_rollout(policy, env)\n",
    "save_rgb_animation(imgs, \"trained.gif\")\n",
    "IImage(filename=\"trained.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
