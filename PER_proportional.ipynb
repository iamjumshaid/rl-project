{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f94ae2763a381c7ffc0ae9ba26921a47",
     "grade": false,
     "grade_id": "cell-dcf2e438bf23416e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 0 Setup\n",
    "These are the same packages as in the last exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f62924e27717275982cfd79c7c5394c",
     "grade": false,
     "grade_id": "cell-df50e80428bfd961",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (0.20.0)\n",
      "Requirement already satisfied: torchaudio in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: gymnasium==0.29.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from gymnasium==0.29.1) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from gymnasium==0.29.1) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from gymnasium==0.29.1) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from gymnasium==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: minatar==1.0.15 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (1.0.15)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (1.4.7)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (2.0.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (2.2.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.9 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (1.13.1)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (0.13.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from minatar==1.0.15) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (1.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (4.54.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (11.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from pandas>=0.24.2->minatar==1.0.15) (2024.2)\n",
      "Requirement already satisfied: matplotlib in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: imageio in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (2.36.0)\n",
      "Requirement already satisfied: numpy in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from imageio) (2.0.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/lilit/miniconda3/envs/lab_project/lib/python3.11/site-packages (from imageio) (11.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install minatar==1.0.15\n",
    "!pip install matplotlib\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5966047618ac56019cf85416e5aa7df",
     "grade": false,
     "grade_id": "cell-56f2e20fd1046476",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from segment_tree import SumSegmentTree, MinSegmentTree\n",
    "\n",
    "from DQN import DQN\n",
    "from utils import make_epsilon_greedy_policy, linear_epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, max_size: int, alpha: float = 0.2):\n",
    "        \"\"\"\n",
    "        Create the replay buffer.\n",
    "\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "        # Maximum priority for storing new samples\n",
    "        self.max_size = max_size\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "        self.pointer = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.buffer = [None] * max_size\n",
    "\n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_size = 1\n",
    "        while tree_size < self.max_size:\n",
    "            tree_size *= 2\n",
    "\n",
    "        self.min_tree = MinSegmentTree(tree_size)\n",
    "        self.sum_tree = SumSegmentTree(tree_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adds a new transition to the buffer. When the buffer is full, overwrite the oldest transition.\n",
    "\n",
    "        :param obs: The current observation.\n",
    "        :param action: The action.\n",
    "        :param reward: The reward.\n",
    "        :param next_obs: The next observation.\n",
    "        :param terminated: Whether the episode terminated.\n",
    "        \"\"\"\n",
    "        idx = self.pointer\n",
    "\n",
    "        transition = (obs, action, reward, next_obs, terminated)\n",
    "        self.buffer[idx] = transition\n",
    "\n",
    "        scaled_priority = self.max_priority ** self.alpha\n",
    "        self.sum_tree[idx] = scaled_priority\n",
    "        self.min_tree[idx] = scaled_priority\n",
    "\n",
    "        self.pointer = (self.pointer + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def update_experience_priorities(self, indices: list[int], priorities: torch.Tensor):\n",
    "        for idx, priority in zip(indices, priorities): \n",
    "            scaled_priority = priority ** self.alpha\n",
    "\n",
    "            self.sum_tree[idx] = scaled_priority\n",
    "            self.min_tree[idx] = scaled_priority\n",
    "\n",
    "            self.max_priority = max(scaled_priority, self.max_priority)\n",
    "\n",
    "    def _sample_indices_by_priority(self, batch_size):\n",
    "        \"\"\"Sample indices proportional to priorities.\"\"\"\n",
    "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
    "        segment_len = p_total / batch_size\n",
    "\n",
    "        indices = []\n",
    "        for i in range(batch_size):\n",
    "            segment_start = segment_len * i\n",
    "            segment_end = segment_len * (i + 1)\n",
    "            sample_idx = self.sum_tree.retrieve(random.uniform(segment_start, segment_end))\n",
    "            indices.append(sample_idx)\n",
    "\n",
    "        return indices\n",
    "    \n",
    "    def _compute_importance_weights(self, indices: list[int], beta: float):\n",
    "        \"\"\"Calculate importance-sampling weights for sampled indices.\"\"\"\n",
    "        p_total = self.sum_tree.sum()\n",
    "        p_min = self.min_tree.min() / p_total\n",
    "\n",
    "        max_weight = (p_min * len(self)) ** -beta\n",
    "\n",
    "        weights = []\n",
    "        for idx in indices:\n",
    "            weight = ((self.sum_tree[idx] / p_total) * len(self)) ** -beta / max_weight\n",
    "            weights.append(weight)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def sample(self, batch_size: int, beta: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
    "\n",
    "        :param batch_size: The batch size.\n",
    "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
    "        \"\"\"\n",
    "        assert len(self) >= batch_size\n",
    "\n",
    "        indices = self._sample_indices_by_priority(batch_size)\n",
    "        weights = self._compute_importance_weights(indices, beta)\n",
    "        transitions = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        obs, actions, rewards, next_obs, terminated = zip(*transitions)\n",
    "\n",
    "        # Convert to torch tensors and stack\n",
    "        obs = torch.stack([o.clone().detach() for o in obs])\n",
    "        actions = torch.tensor(actions)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        next_obs = torch.stack([no.clone().detach() for no in next_obs])\n",
    "        terminated = torch.tensor(terminated)\n",
    "        weights = torch.tensor(weights)\n",
    "\n",
    "        return obs, actions, rewards, next_obs, terminated, indices, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dqn(\n",
    "        q: nn.Module,\n",
    "        q_target: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        gamma: float,\n",
    "        obs: torch.Tensor,\n",
    "        act: torch.Tensor,\n",
    "        rew: torch.Tensor,\n",
    "        next_obs: torch.Tensor,\n",
    "        tm: torch.Tensor,\n",
    "        indices: list,\n",
    "        weights: torch.Tensor,\n",
    "        memory: PrioritizedReplayBuffer,\n",
    "        priority_eps: float = 1e-6,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update the DQN network for one optimizer step.\n",
    "\n",
    "    :param q: The DQN network.\n",
    "    :param q_target: The target DQN network.\n",
    "    :param optimizer: The optimizer.\n",
    "    :param gamma: The discount factor.\n",
    "    :param obs: Batch of current observations.\n",
    "    :param act: Batch of actions.\n",
    "    :param rew: Batch of rewards.\n",
    "    :param next_obs: Batch of next observations.\n",
    "    :param tm: Batch of termination flags.\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate the TD-Target\n",
    "    with torch.no_grad():\n",
    "        q_s_prime = q_target(next_obs)\n",
    "\n",
    "        max_q_s_prime = torch.max(q_s_prime, dim=1)[0]\n",
    "        td_target = rew + gamma * max_q_s_prime * ~tm\n",
    "\n",
    "    # Calculate the loss. Hint: Pytorch has the \".gather()\" function, which collects values along a specified axis using some specified indexes\n",
    "    q_s_a = torch.gather(q(obs), dim=1, index=act.unsqueeze(1)).squeeze(1)\n",
    "    elementwise_loss = F.smooth_l1_loss(q_s_a, td_target, reduction=\"none\")\n",
    "    \n",
    "    loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "    # Backpropagate the loss and step the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    td_delta = elementwise_loss.detach().cpu().numpy()\n",
    "    new_priorities = td_delta + priority_eps\n",
    "    memory.update_experience_priorities(indices, new_priorities)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2567fe20dcb19040977c7b7c1f87983",
     "grade": true,
     "grade_id": "cell-ea9e6a4c5d20b294",
     "locked": false,
     "points": 3.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "            env,\n",
    "            gamma=0.99,\n",
    "            lr=0.001, \n",
    "            batch_size=64,\n",
    "            eps_start=1.0,\n",
    "            eps_end=0.1,\n",
    "            schedule_duration=10_000,\n",
    "            update_freq=100,\n",
    "            maxlen=100_000,\n",
    "            alpha=0.2,\n",
    "            beta=0.6,\n",
    "            squared_grad_momentum=0.95,\n",
    "            min_squared_grad=0.01\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent.\n",
    "\n",
    "        :param env: The environment.\n",
    "        :param gamma: The discount factor.\n",
    "        :param lr: The learning rate.\n",
    "        :param batch_size: Mini batch size.\n",
    "        :param eps_start: The initial epsilon value.\n",
    "        :param eps_end: The final epsilon value.\n",
    "        :param schedule_duration: The duration of the schedule (in timesteps).\n",
    "        :param update_freq: How often to update the Q target.\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.schedule_duration = schedule_duration\n",
    "        self.update_freq = update_freq\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the Replay Buffer\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(maxlen, alpha)\n",
    "\n",
    "        # Initialize the Deep Q-Network. Hint: Remember observation_space and action_space\n",
    "        self.q = DQN(self.env.observation_space.shape, self.env.action_space.n)\n",
    "\n",
    "        # Initialize the second Q-Network, the target network. Load the parameters of the first one into the second\n",
    "        self.q_target = DQN(self.env.observation_space.shape, self.env.action_space.n)\n",
    "        self.q_target.load_state_dict(self.q.state_dict())\n",
    "\n",
    "        # Create an ADAM optimizer for the Q-network\n",
    "        self.optimizer = optim.RMSprop(self.q.parameters(), lr=lr, alpha=squared_grad_momentum, centered=True, eps=min_squared_grad)\n",
    "\n",
    "        self.policy = make_epsilon_greedy_policy(self.q, env.action_space.n)\n",
    "\n",
    "\n",
    "    def train(self, num_episodes: int) -> EpisodeStats:\n",
    "        \"\"\"\n",
    "        Train the DQN agent.\n",
    "\n",
    "        :param num_episodes: Number of episodes to train.\n",
    "        :returns: The episode statistics.\n",
    "        \"\"\"\n",
    "        # Keeps track of useful statistics\n",
    "        stats = EpisodeStats(\n",
    "            episode_lengths=np.zeros(num_episodes),\n",
    "            episode_rewards=np.zeros(num_episodes),\n",
    "        )\n",
    "        current_timestep = 0\n",
    "        epsilon = self.eps_start\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Print out which episode we're on, useful for debugging.\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}  Epsilon: {epsilon:.3f}')\n",
    "\n",
    "            # Reset the environment and get initial observation\n",
    "            obs, _ = self.env.reset()\n",
    "            \n",
    "            for episode_time in itertools.count():\n",
    "                # Get current epsilon value\n",
    "                epsilon = linear_epsilon_decay(self.eps_start, self.eps_end, current_timestep, self.schedule_duration)\n",
    "\n",
    "                # Choose action and execute\n",
    "                action = self.policy(torch.as_tensor(obs).unsqueeze(0).float(), epsilon=epsilon)\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Update statistics\n",
    "                stats.episode_rewards[i_episode] += reward\n",
    "                stats.episode_lengths[i_episode] += 1\n",
    "\n",
    "                # Store sample in the replay buffer\n",
    "                self.replay_buffer.store(torch.tensor(obs),\n",
    "                                         torch.tensor(action),\n",
    "                                         torch.tensor(reward),\n",
    "                                         torch.tensor(next_obs),\n",
    "                                         torch.tensor(terminated))\n",
    "                \n",
    "                # Linearly increase beta \n",
    "                fraction = min(i_episode / num_episodes, 1.0)\n",
    "                self.beta = self.beta + fraction * (1.0 - self.beta)\n",
    "\n",
    "                # Sample a mini batch from the replay buffer\n",
    "                \n",
    "                if len(self.replay_buffer) > self.batch_size:\n",
    "                    obs_batch, act_batch, rew_batch, next_obs_batch, tm_batch, indices, weights = self.replay_buffer.sample(self.batch_size, self.beta)\n",
    "\n",
    "                    # Update the Q network\n",
    "                    update_dqn(\n",
    "                        self.q,\n",
    "                        self.q_target,\n",
    "                        self.optimizer,\n",
    "                        self.gamma, \n",
    "                        obs_batch.float(),\n",
    "                        act_batch, \n",
    "                        rew_batch.float(),\n",
    "                        next_obs_batch.float(),\n",
    "                        tm_batch,\n",
    "                        indices,\n",
    "                        weights,\n",
    "                        self.replay_buffer\n",
    "                    )\n",
    "\n",
    "                    # Update the current Q target\n",
    "                    if current_timestep % self.update_freq == 0:\n",
    "                        self.q_target.load_state_dict(self.q.state_dict())\n",
    "                    current_timestep += 1\n",
    "\n",
    "                # Check whether the episode is finished\n",
    "                if terminated or truncated or episode_time >= 500:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5c43245162e35823e014e7d35aab00b",
     "grade": false,
     "grade_id": "cell-a177045dee6d0c56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on MinAtar/Asterix-v1\n",
      "Observation space: Box(False, True, (10, 10, 4), bool)\n",
      "Action space: Discrete(5)\n",
      "\n",
      "Episode 100 of 1000  Time Step: 6401  Epsilon: 0.616\n",
      "Episode 200 of 1000  Time Step: 13472  Epsilon: 0.192\n",
      "Episode 300 of 1000  Time Step: 21511  Epsilon: 0.100\n",
      "Episode 400 of 1000  Time Step: 28537  Epsilon: 0.100\n",
      "Episode 500 of 1000  Time Step: 36103  Epsilon: 0.100\n",
      "Episode 600 of 1000  Time Step: 43472  Epsilon: 0.100\n",
      "Episode 700 of 1000  Time Step: 50561  Epsilon: 0.100\n",
      "Episode 800 of 1000  Time Step: 57785  Epsilon: 0.100\n"
     ]
    }
   ],
   "source": [
    "# Choose your environment\n",
    "env = gym.make('MinAtar/Asterix-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# Print observation and action space infos\n",
    "print(f\"Training on {env.spec.id}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\\n\")\n",
    "\n",
    "# Hyperparameters, Hint: Change as you see fit\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "REPLAY_BUFFER_SIZE = 500_000\n",
    "UPDATE_FREQ = 100\n",
    "EPS_START = 1\n",
    "EPS_END = 0.1\n",
    "SCHEDULE_DURATION = 15_000\n",
    "NUM_EPISODES = 1_000\n",
    "\n",
    "ALPHA = 0.5\n",
    "BETA = 0\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "# Train DQN\n",
    "agent = DQNAgent(\n",
    "    env, \n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    eps_start=EPS_START,\n",
    "    eps_end=EPS_END,\n",
    "    schedule_duration=SCHEDULE_DURATION,\n",
    "    update_freq=UPDATE_FREQ,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    "    alpha=ALPHA,\n",
    "    beta=BETA,\n",
    ")\n",
    "stats = agent.train(NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m smoothing_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[0;32m----> 2\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m), tight_layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the episode length over time\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ax \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "smoothing_window=500\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "# Plot the episode length over time\n",
    "ax = axes[0]\n",
    "ax.plot(stats.episode_lengths)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Length\")\n",
    "ax.set_title(\"Episode Length over Time\") \n",
    "\n",
    "# Plot the episode reward over time\n",
    "ax = axes[1]\n",
    "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "ax.plot(rewards_smoothed)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
    "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing_window=500\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "# # Plot the episode length over time\n",
    "# ax = axes[0]\n",
    "# ax.plot(stats.episode_lengths)\n",
    "# ax.set_xlabel(\"Episode\")\n",
    "# ax.set_ylabel(\"Episode Length\")\n",
    "# ax.set_title(\"Episode Length over Time\") \n",
    "\n",
    "# # Plot the episode reward over time\n",
    "# ax = axes[1]\n",
    "# rewards_smoothed = df[\"reward\"].rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "# ax.plot(rewards_smoothed)\n",
    "# ax.set_xlabel(\"Episode\")\n",
    "# ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
    "# ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results/freeway_dqn_stats.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../results/freeway_dqn_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(stats\u001b[38;5;241m.\u001b[39mepisode_lengths) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m: stats\u001b[38;5;241m.\u001b[39mepisode_lengths,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m: stats\u001b[38;5;241m.\u001b[39mepisode_rewards\n\u001b[1;32m      5\u001b[0m })\n\u001b[1;32m      7\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results/freeeway_per_stats.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(save_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"episode\": range(1, len(stats.episode_lengths) + 1),\n",
    "    \"length\": stats.episode_lengths,\n",
    "    \"reward\": stats.episode_rewards\n",
    "})\n",
    "\n",
    "save_path = \"../results/freeeway_per_stats.csv\"\n",
    "\n",
    "df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbf5868f2d439f8bf634d4fe2267cfad",
     "grade": false,
     "grade_id": "cell-dbcfd10b74c6d50e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_epsilon_greedy_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m imgs\n\u001b[0;32m---> 33\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mmake_epsilon_greedy_policy\u001b[49m(agent\u001b[38;5;241m.\u001b[39mq, num_actions\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m     34\u001b[0m imgs \u001b[38;5;241m=\u001b[39m rendered_rollout(policy, env)\n\u001b[1;32m     35\u001b[0m save_rgb_animation(imgs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_epsilon_greedy_policy' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image as IImage\n",
    "\n",
    "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
    "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
    "    # Create a list to hold each frame\n",
    "    frames = []\n",
    "\n",
    "    # Convert RGB arrays to PIL Image objects\n",
    "    for rgb_array in rgb_arrays:\n",
    "        rgb_array = (rgb_array*255).astype(np.uint8)\n",
    "        rgb_array = rgb_array.repeat(48, axis=0).repeat(48, axis=1)\n",
    "        img = Image.fromarray(rgb_array)\n",
    "        frames.append(img)\n",
    "\n",
    "    # Save the frames as an animated GIF\n",
    "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "\n",
    "def rendered_rollout(policy, env, max_steps=1_000):\n",
    "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = [env.render()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = policy(torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0))\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        imgs.append(env.render())\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "policy = make_epsilon_greedy_policy(agent.q, num_actions=env.action_space.n)\n",
    "imgs = rendered_rollout(policy, env)\n",
    "save_rgb_animation(imgs, \"trained.gif\")\n",
    "IImage(filename=\"trained.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e59e697be38c22a3f07983f16f4dd7c",
     "grade": false,
     "grade_id": "cell-2f976a8294047887",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have not trained very far because it can be very time consuming. But feel free to change the hyperparameters and try to improve your results!\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **The deadly triad** (2 points)\n",
    "\n",
    "In Sutton and Barto's \"Reinforcement Learning: An Introduction\" they introduced the concept of the **Deadly Triad**. Investigate this concept and explain in your own words what it describes. What does it say about DQNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4b100e91dd1e1baa4a3cf1f420ac900",
     "grade": true,
     "grade_id": "cell-c732353a7944c3d8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answers here:**\n",
    "\n",
    "The Deadly triad consists of three factors: Bootstrapping, Function Approximation, and Off-Policy learning. The three combined have the danger of divergence. With bootstrapping you use the estimated values of next states to update the current state, which can become problematic, as the weights of the network are updated simultaneously for every state with function approximation. When using Off-Policy learning, the agent might learn on actions that the target would take infrequently and when expectations are increased, they can be left unchecked, blowing off and leading to divergence. DQNs use all three. \n",
    "\n",
    "In the book it is mentioned that if the behavior policy is sufficiently close to the target, like in the case of epsilon-greedy, Q-learning is not found to diverge. They then discuss using methods of avoiding the deadly triad such as using MC to avoid Bootstrapping or Sarsa to be On-Policy. One can also use Double Q-learning, Target Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d30fa2e22cd4d2da2fd62610c4403b11",
     "grade": false,
     "grade_id": "cell-6d3f59695aec105d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Going beyond discrete action spaces** (1.5 points)\n",
    "\n",
    "1. What is the difference between discrete and continuous action spaces?\n",
    "2. Name a key problem that may arise by using DQN for continuous action spaces. Hint: Think about how we select a greedy action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4f54bdee1c58604a05406b39d2db66e",
     "grade": true,
     "grade_id": "cell-4780e5a781b693fa",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answers here:**\n",
    "1. One chooses a discrete action and the other has continous space. Like choosing Left or Right or choosing how much Left or Right.\n",
    "2. When you take the max over actions, it is easy to for discrete actions as you can enumerate and iterate, which you can't do for the continous case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae4827a587c050c56de3f248da873f19",
     "grade": false,
     "grade_id": "cell-73b6ef9d83c95855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "<span style=\"color:orange\">**FEEDBACK**</span>\n",
    "\n",
    "Describe your experience with this exercise and the course. We have set up an anonymous option for this in ILIAS or you can write your feedback directly here in the notebook. Please note that your feedback will help us to improve the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really enjoyed this exercise. It was the right amount of challenge, in my opinion, at least for me. I think a test for the update_dqn function would be a nice addition, as it can fail silently, due to the broadcasting of torch tensors. This was at least the case for me, but luckily the bug was found quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
